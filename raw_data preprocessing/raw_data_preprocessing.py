# -*- coding: utf-8 -*-
"""Raw Data Preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rLTxQlfzWjJJtypdhtYp6T538-GpIH9m
"""

import pandas as pd
import numpy as np

#Hyperparameters
batch_size = 64
latent_dim = 256
num_samples = 10000

#Vectorize the data.
input_texts = []
target_texts = []
input_chars = set()
target_chars = set()

with open('fra.txt', 'r', encoding='utf-8') as f:
    lines = f.read().split('\n')
for line in lines[: min(num_samples, len(lines) - 1)]:
    input_text, target_text = line.split('\t')
    input_texts.append(input_text)
    target_texts.append(target_text)
    for char in input_text:
        if char not in input_chars:
            input_chars.add(char)
    for char in target_text:
        if char not in target_chars:
            target_chars.add(char)

input_chars = sorted(list(input_chars))
target_chars = sorted(list(target_chars))
num_encoder_tokens = len(input_chars)
num_decoder_tokens = len(target_chars)
max_encoder_seq_length = max([len(txt) for txt in input_texts])
max_decoder_seq_length = max([len(txt) for txt in target_texts])

#Print size
print('Number of samples:', len(input_texts))
print('Number of unique input tokens:', num_encoder_tokens)
print('Number of unique output tokens:', num_decoder_tokens)
print('Max sequence length for inputs:', max_encoder_seq_length)
print('Max sequence length for outputs:', max_decoder_seq_length)

import re
import random

def clean_text(text):
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\u202f','',text)
    # Lowercase text
    text = text.lower()
    return text

def split_data(data, split_ratios):
    # Shuffle data
    random.shuffle(data)
    # Calculate split indices
    training_split = int(split_ratios[0] * len(data))
    validation_split = int(split_ratios[1] *len(data)) + training_split
    # Split data into training, validation, and test sets
    training_data = data[:training_split]
    validation_data = data[training_split:validation_split]
    test_data = data[validation_split:]
    return training_data, validation_data, test_data

def create_dataset(src_texts, tgt_texts, split_ratios):
    # Clean text
    src_texts = [clean_text(text) for text in src_texts]
    tgt_texts = [clean_text(text) for text in tgt_texts]
    data = list(zip(src_texts, tgt_texts))
    # Split data
    training_data, validation_data, test_data = split_data(data, split_ratios)
    return training_data, validation_data, test_data

split_ratios = [0.8, 0.1]
train,test,validation = create_dataset(input_texts,target_texts,split_ratios)

train_en = [text[0] for text in train]
train_fr = [text[1] for text in train]

test_en = [text[0] for text in test]
test_fr = [text[1] for text in test]
validation_en = [text[0] for text in validation]
validation_fr = [text[1] for text in validation]

with open('train_en.txt', 'w') as fp:
    for item in train_en:
        # write each item on a new line
        fp.write("%s\n" % item)
fp.close()
with open('train_fr.txt','w') as f:
  for line in train_fr:
    f.write("%s\n" % line)
f.close()

with open('test_en.txt','w') as fp:
  for line in test_en:
    fp.write("%s\n"%line)
fp.close()
with open('test_fr.txt','w') as fp:
  for line in test_fr:
    fp.write("%s\n"%line)
fp.close()
with open('validation_en.txt','w') as fp:
  for line in validation_en:
    fp.write("%s\n"%line)
fp.close()
with open('validation_fr.txt','w') as fp:
  for line in validation_fr:
    fp.write("%s\n"%line)
fp.close()

from zipfile import ZipFile

with ZipFile('train.zip', 'w') as zip_object:
   # Adding files that need to be zipped
   zip_object.write('train_en.txt')
   zip_object.write('train_fr.txt')
zip_object.close()

with ZipFile('test.zip', 'w') as zip_object:
   # Adding files that need to be zipped
   zip_object.write('test_en.txt')
   zip_object.write('test_fr.txt')
zip_object.close()
with ZipFile('validation.zip', 'w') as zip_object:
   # Adding files that need to be zipped
   zip_object.write('validation_en.txt')
   zip_object.write('validation_fr.txt')
zip_object.close()